# Q-Learningのお勉強、めも

### CartPoleについて
OpenAIGymというライブラリの代表的な問題、カートの上に棒が乗っている！   
状態の要素は
* カート位置 -2.4～2.4
* カート速度 -3.0～3.0
* 棒の角度 　-41.8～41.8
* 棒の角速度　-2.0～2.0

こんな感じ。カートが行き過ぎちゃったり、角度が20.9以上いくとダメみたい。    
カートを右か左に押す行動のみをして、200step維持出来たらOK!    

### Q学習
Q学習では、各状態sで最適な行動aを与える関数A(s)を求める代わりに、sで行動aをするとこの先どのくらいの報酬が合計で貰えるか示す    
行動価値関数Q(s,a)=R(t)   
を求める、らしいうーん？

#### 今回の手法
CartPoleの場合、各状態sってのは上の位置、速度、角度、角速度の4つ。それぞれ連続した変数として扱う。まあ飛ばないしね。   
で、6分割するらしい。これが離散化。その結果、6**4で1296の状態を定義するから、   
Q関数は[1296×2]の行列（表）で表す。
（2は選択可能な行動で、右に押すか左に押すかを表す）
これで状態と行動の表でQ関数を表すってことなのかな。

報酬は、「200step立ち続ける」と「途中の各stepで立っている」ことに対して与える。   
こけたらマイナスの報酬。

#### 学習方法
こけた時点のQ(s,a)にマイナスの報酬を与えて、それ以前のQ(s-1,a-1)みたいなのにも、少なめのマイナス報酬を与えていくみたい。

Q関数の学習は、    
```
Q(s_t, a_t) ← Q(s_t, a_t) + α(r_{t}+γMAX{Q(s_{t+1}, a_{t+1})} – Q(s_t, a_t))
```
で示されるようだ。MAX{...}は次の時間t+1から先にもらえる報酬合計の最大値。αは学習率。更新の大きさを決定する。    
この式が言いたいことは、    
```
時刻tで状態s_tであったときに、行動a_tを取ったときにその後得られる報酬の合計R(t)を与える関数Q(s_t, a_t)は、実際に時刻tでもらった報酬r_{t}と、そのさきにもらえるであろう報酬R(t+1)の最大値であるMAX{Q(s_{t+1}, a_{t+1})}に割引率γを掛けた値、の和で表される。
その値に近づくように少しずつ更新しよう♪
```
うーん分かるような分からないような。

### 探索と利用のジレンマ
常に良い感じなaを選択し続けてると、一部の選択肢しか辿らないような気がする。その解決策として、   
ε_0*(1/episode)と表される確率ε以下なら、ランダムなaを選ぶ。これによって、試行数が低いうちはランダムな行動を試せて、試行数が増えるに従って安定してくる。    
まあ普通といえば普通な気はする。
